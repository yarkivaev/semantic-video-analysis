{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoItu8jcs-yC",
        "outputId": "93fcba46-eafa-45f8-a7aa-9070a1bfc7f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vosk in /usr/local/lib/python3.11/dist-packages (0.3.45)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from vosk) (1.17.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vosk) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vosk) (4.67.1)\n",
            "Requirement already satisfied: srt in /usr/local/lib/python3.11/dist-packages (from vosk) (3.5.3)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.11/dist-packages (from vosk) (15.0.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->vosk) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vosk) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vosk) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vosk) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vosk) (2025.7.9)\n",
            "--2025-07-13 05:57:00--  https://alphacephei.com/vosk/models/vosk-model-ru-0.22.zip\n",
            "Resolving alphacephei.com (alphacephei.com)... 188.40.21.16, 2a01:4f8:13a:279f::2\n",
            "Connecting to alphacephei.com (alphacephei.com)|188.40.21.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1559419132 (1.5G) [application/zip]\n",
            "Saving to: ‘vosk-model-ru-0.22.zip.1’\n",
            "\n",
            "vosk-model-ru-0.22. 100%[===================>]   1.45G  23.4MB/s    in 64s     \n",
            "\n",
            "2025-07-13 05:58:05 (23.4 MB/s) - ‘vosk-model-ru-0.22.zip.1’ saved [1559419132/1559419132]\n",
            "\n",
            "Archive:  vosk-model-ru-0.22.zip\n",
            "replace vosk-model-ru-0.22/graph/words.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!pip install vosk\n",
        "!wget https://alphacephei.com/vosk/models/vosk-model-ru-0.22.zip\n",
        "!unzip vosk-model-ru-0.22.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fMffUtq9t5aH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "# import whisper\n",
        "import torch\n",
        "import json\n",
        "from PIL import Image\n",
        "from moviepy.editor import VideoFileClip\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from datetime import datetime\n",
        "from vosk import Model, KaldiRecognizer\n",
        "import wave\n",
        "import json as js\n",
        "import os\n",
        "\n",
        "\n",
        "# Setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "os.makedirs(\"extracted_frames\", exist_ok=True)\n",
        "\n",
        "# Load models\n",
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "# whisper_model = whisUntitled7per.load_model(\"base\")  # or 'small'/'medium'\n",
        "\n",
        "# --------- Frame Extraction ----------\n",
        "def extract_key_frames(video_path, num_frames=5):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    indices = [int(i * total / num_frames) for i in range(num_frames)]\n",
        "    paths = []\n",
        "    for idx, f in enumerate(indices):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, f)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "            path = f\"extracted_frames/{name}_frame{idx}.jpg\"\n",
        "            cv2.imwrite(path, frame)\n",
        "            paths.append(path)\n",
        "    cap.release()\n",
        "    return paths\n",
        "\n",
        "# --------- Visual Captioning ----------\n",
        "def generate_caption(img_path):\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    inputs = blip_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        out = blip_model.generate(**inputs)\n",
        "    return blip_processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "# # --------- Audio Transcription ----------\n",
        "# def transcribe_audio(video_path):\n",
        "#     temp_audio = \"temp_audio.wav\"\n",
        "#     video = VideoFileClip(video_path)\n",
        "#     video.audio.write_audiofile(temp_audio, verbose=False, logger=None)\n",
        "#     result = whisper_model.transcribe(temp_audio)\n",
        "#     os.remove(temp_audio)\n",
        "#     return result[\"text\"], result.get(\"segments\", [])\n",
        "\n",
        "# --------- File Metadata ----------\n",
        "def get_file_info(path):\n",
        "    stat = os.stat(path)\n",
        "    return stat.st_size, datetime.utcfromtimestamp(stat.st_ctime).strftime('%Y-%m-%d %H:%M:%S UTC')\n",
        "\n",
        "# --------- Combined Description ----------\n",
        "def describe_video_multimodal(video_path, num_frames=5):\n",
        "    # File metadata\n",
        "    file_name = os.path.basename(video_path)\n",
        "    file_path = os.path.abspath(video_path)\n",
        "    file_size, created_at = get_file_info(video_path)\n",
        "    clip = VideoFileClip(video_path)\n",
        "    width, height = clip.size\n",
        "    duration = clip.duration\n",
        "    fps = clip.fps\n",
        "    has_audio = clip.audio is not None\n",
        "    fmt = file_name.split(\".\")[-1]\n",
        "\n",
        "    # Visual description\n",
        "    frame_paths = extract_key_frames(video_path, num_frames)\n",
        "    visual_captions = [generate_caption(p) for p in frame_paths]\n",
        "\n",
        "    # Audio transcription\n",
        "    audio_text, audio_segments = transcribe_audio_vosk(video_path) if has_audio else (\"\", [])\n",
        "    print(\"AUDIO\", audio_text)\n",
        "    # Merge content\n",
        "    combined_desc = \". \".join(set(visual_captions)) + (\". Spoken: \" + audio_text if audio_text else \"\")\n",
        "\n",
        "    return {\n",
        "        \"type\": \"video\",\n",
        "        \"metadata\": {\n",
        "            \"fileName\": file_name,\n",
        "            \"filePath\": file_path,\n",
        "            \"fileSize\": file_size,\n",
        "            \"createdAt\": created_at,\n",
        "            \"description\": combined_desc[:150] + \"...\",\n",
        "            \"tags\": list(set(word for cap in visual_captions for word in cap.lower().split() if word.isalpha()))[:5]\n",
        "        },\n",
        "        \"duration\": duration,\n",
        "        \"resolution\": {\"width\": width, \"height\": height},\n",
        "        \"frameRate\": fps,\n",
        "        \"hasAudio\": has_audio,\n",
        "        \"videoFormat\": fmt,\n",
        "        \"contentAnalysis\": {\n",
        "            \"contentOverview\": combined_desc,\n",
        "            \"actionIntroduction\": visual_captions[0],\n",
        "            \"visualCaptions\": visual_captions,\n",
        "            \"audioTranscript\": audio_text,\n",
        "            \"audioSegments\": audio_segments,\n",
        "            \"estimatedMood\": \"neutral\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "# Load model once (globally)\n",
        "vosk_model = Model(\"vosk-model-ru-0.22\")\n",
        "\n",
        "def transcribe_audio_vosk(video_path):\n",
        "    temp_audio = \"temp_audio.wav\"\n",
        "    video = VideoFileClip(video_path)\n",
        "    video.audio.write_audiofile(temp_audio, verbose=False, logger=None)\n",
        "\n",
        "    wf = wave.open(temp_audio, \"rb\")\n",
        "    rec = KaldiRecognizer(vosk_model, wf.getframerate())\n",
        "    rec.SetWords(True)\n",
        "\n",
        "    result_text = \"\"\n",
        "    segments = []\n",
        "\n",
        "    while True:\n",
        "        data = wf.readframes(4000)\n",
        "        if len(data) == 0:\n",
        "            break\n",
        "        if rec.AcceptWaveform(data):\n",
        "            part = js.loads(rec.Result())\n",
        "            if \"text\" in part:\n",
        "                result_text += part[\"text\"] + \" \"\n",
        "                segments.append(part)\n",
        "    wf.close()\n",
        "    os.remove(temp_audio)\n",
        "    return result_text.strip(), segments\n"
      ],
      "metadata": {
        "id": "itB86YLctKF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "918ed421-8195-477c-99c7-5390903486f1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0kHMzaueuv_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# video_paths = [\"1.3.mp4\", \"2.5.mp4\", \"face.mp4\"]\n",
        "video_paths = [\"tg_video_13_07.mp4\"]\n",
        "\n",
        "for path in video_paths:\n",
        "    print(f\"\\nProcessing: {path}\")\n",
        "    try:\n",
        "        result = describe_video_multimodal(path)\n",
        "        with open(f\"{os.path.splitext(os.path.basename(path))[0]}_multimodal_description.json\", \"w\") as f:\n",
        "            json.dump(result, f, indent=2)\n",
        "        print(\"✅ Done:\", result['metadata']['description'])\n",
        "    except Exception as e:\n",
        "        print(\"❌ Error:\", e)\n"
      ],
      "metadata": {
        "id": "c0IOKLykuBj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e819b7a-11a5-4c49-c8d8-1936f6cfc3ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing: tg_video_13_07.mp4\n",
            "AUDIO что да ты наташа что это почему же  сядь для сайт пошёл ты\n",
            "✅ Done: a person sitting in a chair with a laptop. a woman with blonde hair and a black shirt. a woman with glasses is taking a self self self self self self ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import json\n",
        "import wave\n",
        "import whisper\n",
        "from PIL import Image\n",
        "from moviepy.editor import VideoFileClip\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from datetime import datetime\n",
        "\n",
        "# ────────────────────────────────\n",
        "# SETUP\n",
        "# ────────────────────────────────\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "os.makedirs(\"extracted_frames\", exist_ok=True)\n",
        "\n",
        "# Load models\n",
        "print(\"Loading models...\")\n",
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "whisper_model = whisper.load_model(\"large-v3\")\n",
        "print(\"Models loaded.\")\n",
        "\n",
        "# ────────────────────────────────\n",
        "# FRAME EXTRACTION\n",
        "# ────────────────────────────────\n",
        "def extract_key_frames(video_path, num_frames=5):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    indices = [int(i * total / num_frames) for i in range(num_frames)]\n",
        "    paths = []\n",
        "    for idx, f in enumerate(indices):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, f)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "            path = f\"extracted_frames/{name}_frame{idx}.jpg\"\n",
        "            cv2.imwrite(path, frame)\n",
        "            paths.append(path)\n",
        "    cap.release()\n",
        "    return paths\n",
        "\n",
        "# ────────────────────────────────\n",
        "# VISUAL CAPTIONING (BLIP)\n",
        "# ────────────────────────────────\n",
        "def generate_caption(img_path):\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    inputs = blip_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        out = blip_model.generate(**inputs)\n",
        "    return blip_processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "# ────────────────────────────────\n",
        "# AUDIO TRANSCRIPTION (Whisper)\n",
        "# ────────────────────────────────\n",
        "def transcribe_audio_whisper(video_path):\n",
        "    temp_audio = \"temp_audio.wav\"\n",
        "    video = VideoFileClip(video_path)\n",
        "    video.audio.write_audiofile(temp_audio, verbose=False, logger=None)\n",
        "\n",
        "    # Transcribe in Russian\n",
        "    result = whisper_model.transcribe(temp_audio, language=\"ru\")\n",
        "    os.remove(temp_audio)\n",
        "\n",
        "    return result[\"text\"], result.get(\"segments\", [])\n",
        "\n",
        "# ────────────────────────────────\n",
        "# FILE INFO UTILS\n",
        "# ────────────────────────────────\n",
        "def get_file_info(path):\n",
        "    stat = os.stat(path)\n",
        "    return stat.st_size, datetime.utcfromtimestamp(stat.st_ctime).strftime('%Y-%m-%d %H:%M:%S UTC')\n",
        "\n",
        "# ────────────────────────────────\n",
        "# FULL VIDEO DESCRIPTION\n",
        "# ────────────────────────────────\n",
        "def describe_video_multimodal(video_path, num_frames=5):\n",
        "    file_name = os.path.basename(video_path)\n",
        "    file_path = os.path.abspath(video_path)\n",
        "    file_size, created_at = get_file_info(video_path)\n",
        "    clip = VideoFileClip(video_path)\n",
        "    width, height = clip.size\n",
        "    duration = clip.duration\n",
        "    fps = clip.fps\n",
        "    has_audio = clip.audio is not None\n",
        "    fmt = file_name.split(\".\")[-1]\n",
        "\n",
        "    print(f\"🖼 Extracting frames from {video_path}...\")\n",
        "    frame_paths = extract_key_frames(video_path, num_frames)\n",
        "    visual_captions = [generate_caption(p) for p in frame_paths]\n",
        "\n",
        "    print(\"🎙 Transcribing audio...\")\n",
        "    audio_text, audio_segments = transcribe_audio_whisper(video_path) if has_audio else (\"\", [])\n",
        "\n",
        "    combined_desc = \". \".join(set(visual_captions))\n",
        "    if audio_text:\n",
        "        combined_desc += f\". Spoken: {audio_text}\"\n",
        "\n",
        "    return {\n",
        "        \"type\": \"video\",\n",
        "        \"metadata\": {\n",
        "            \"fileName\": file_name,\n",
        "            \"filePath\": file_path,\n",
        "            \"fileSize\": file_size,\n",
        "            \"createdAt\": created_at,\n",
        "            \"description\": combined_desc[:150] + \"...\",\n",
        "            \"tags\": list(set(word for cap in visual_captions for word in cap.lower().split() if word.isalpha()))[:5]\n",
        "        },\n",
        "        \"duration\": duration,\n",
        "        \"resolution\": {\"width\": width, \"height\": height},\n",
        "        \"frameRate\": fps,\n",
        "        \"hasAudio\": has_audio,\n",
        "        \"videoFormat\": fmt,\n",
        "        \"contentAnalysis\": {\n",
        "            \"contentOverview\": combined_desc,\n",
        "            \"actionIntroduction\": visual_captions[0] if visual_captions else \"\",\n",
        "            \"visualCaptions\": visual_captions,\n",
        "            \"audioTranscript\": audio_text,\n",
        "            \"audioSegments\": audio_segments,\n",
        "            \"estimatedMood\": \"neutral\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "# ────────────────────────────────\n",
        "# PROCESS VIDEOS\n",
        "# ────────────────────────────────\n",
        "video_paths = [\"tg_video_13_07.mp4\"]  # Replace with your own\n",
        "\n",
        "for path in video_paths:\n",
        "    print(f\"\\n📦 Processing: {path}\")\n",
        "    try:\n",
        "        result = describe_video_multimodal(path)\n",
        "        with open(f\"{os.path.splitext(os.path.basename(path))[0]}_multimodal_description.json\", \"w\") as f:\n",
        "            json.dump(result, f, indent=2, ensure_ascii=False)\n",
        "        print(\"✅ Done:\", result['metadata']['description'])\n",
        "    except Exception as e:\n",
        "        print(\"❌ Error:\", e)\n"
      ],
      "metadata": {
        "id": "mUkt5LhSuW9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "281acec5-8322-47bb-8248-9a5793e3ba72"
      },
      "execution_count": 1,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading models...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models loaded.\n",
            "\n",
            "📦 Processing: tg_video_13_07.mp4\n",
            "🖼 Extracting frames from tg_video_13_07.mp4...\n",
            "🎙 Transcribing audio...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Done: a woman with a black shirt and a white shirt. a woman with glasses is taking a self self self self self self self self self self self self self. a man...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!sudo apt-get install -y ffmpeg\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6M7ZPUgZ27R",
        "outputId": "681c8f54-bc53-4e4d-9ada-c583b9c270d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import json\n",
        "import wave\n",
        "import whisper\n",
        "from PIL import Image\n",
        "from moviepy.editor import VideoFileClip\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from datetime import datetime\n",
        "\n",
        "# ────────────────────────────────\n",
        "# SETUP\n",
        "# ────────────────────────────────\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "os.makedirs(\"extracted_frames\", exist_ok=True)\n",
        "\n",
        "\n",
        "whisper_model = whisper.load_model(\"large-v3\")\n",
        "print(\"Models loaded.\")\n",
        "\n",
        "\n",
        "def transcribe_audio_whisper(video_path):\n",
        "    temp_audio = \"temp_audio.wav\"\n",
        "    video = VideoFileClip(video_path)\n",
        "    video.audio.write_audiofile(temp_audio, verbose=False, logger=None)\n",
        "\n",
        "    # Transcribe in Russian\n",
        "    result = whisper_model.transcribe(temp_audio, language=\"ru\")\n",
        "    os.remove(temp_audio)\n",
        "\n",
        "    return result[\"text\"], result.get(\"segments\", [])\n",
        "\n",
        "\n",
        "audio_path = \"tg_video_13_07.mp4\"\n",
        "transcribe_audio_whisper(audio_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAiETWuyZ6jO",
        "outputId": "495446aa-9a90-44ef-e5a5-9a629b1ad87d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models loaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(' скажи что-нибудь по-русски вон там код выполняет мое приложение используем себе протокол таким образом мне не приходится четко определять pipeline выполнение код код сам определяет в какой последовательности нужно что выполнять круто',\n",
              " [{'id': 0,\n",
              "   'seek': 0,\n",
              "   'start': 0.0,\n",
              "   'end': 12.540000000000001,\n",
              "   'text': ' скажи что-нибудь по-русски вон там код выполняет мое приложение используем',\n",
              "   'tokens': [50365,\n",
              "    21938,\n",
              "    435,\n",
              "    2143,\n",
              "    12,\n",
              "    23561,\n",
              "    2801,\n",
              "    12,\n",
              "    2711,\n",
              "    461,\n",
              "    47399,\n",
              "    740,\n",
              "    1784,\n",
              "    8223,\n",
              "    981,\n",
              "    1435,\n",
              "    34771,\n",
              "    5151,\n",
              "    1094,\n",
              "    1084,\n",
              "    5805,\n",
              "    47251,\n",
              "    5627,\n",
              "    15552,\n",
              "    33096,\n",
              "    50992],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.187332245019766,\n",
              "   'compression_ratio': 1.664804469273743,\n",
              "   'no_speech_prob': 0.015318616293370724},\n",
              "  {'id': 1,\n",
              "   'seek': 0,\n",
              "   'start': 12.540000000000001,\n",
              "   'end': 23.16,\n",
              "   'text': ' себе протокол таким образом мне не приходится четко определять pipeline выполнение код код',\n",
              "   'tokens': [50992,\n",
              "    16683,\n",
              "    15602,\n",
              "    2637,\n",
              "    1227,\n",
              "    31584,\n",
              "    29916,\n",
              "    8531,\n",
              "    1725,\n",
              "    26641,\n",
              "    8254,\n",
              "    38140,\n",
              "    3752,\n",
              "    39305,\n",
              "    8773,\n",
              "    15517,\n",
              "    34771,\n",
              "    489,\n",
              "    5627,\n",
              "    981,\n",
              "    1435,\n",
              "    981,\n",
              "    1435,\n",
              "    51523],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.187332245019766,\n",
              "   'compression_ratio': 1.664804469273743,\n",
              "   'no_speech_prob': 0.015318616293370724},\n",
              "  {'id': 2,\n",
              "   'seek': 2316,\n",
              "   'start': 23.16,\n",
              "   'end': 29.02,\n",
              "   'text': ' сам определяет в какой последовательности нужно что выполнять круто',\n",
              "   'tokens': [50365,\n",
              "    5602,\n",
              "    26961,\n",
              "    15292,\n",
              "    1094,\n",
              "    740,\n",
              "    16898,\n",
              "    19253,\n",
              "    1055,\n",
              "    10272,\n",
              "    13975,\n",
              "    12264,\n",
              "    2143,\n",
              "    34771,\n",
              "    35824,\n",
              "    26970,\n",
              "    860,\n",
              "    50658],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11126971244812012,\n",
              "   'compression_ratio': 1.3125,\n",
              "   'no_speech_prob': 0.00047104194527491927}])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FhI4uI_zBupK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}